{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKJTL3Z8gBKHMQGJ0WCUZ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/CoreLlama/blob/main/positional_encoding_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OXoEK90fFu7p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary value function"
      ],
      "metadata": {
        "id": "A7SoC-wvJ0BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_binary(value):\n",
        "\n",
        "    if value == 1 or value == 0 :\n",
        "        return f\"{value}\"\n",
        "\n",
        "    binary = f\"{_get_binary(value//2)}{value%2}\"\n",
        "\n",
        "    return binary\n",
        "\n",
        "bin = _get_binary(16)\n",
        "print(bin)\n",
        "bin_list = list(map(int,bin))\n",
        "tensor = torch.tensor(bin_list)\n",
        "print(tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF7nV0uiHYYf",
        "outputId": "ae5457fc-3021-4cda-a564-6a4741bd0b9e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "tensor([1, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros = torch.zeros((4,6))\n",
        "zeros[2,6-3:] = torch.tensor([1,2,3])\n",
        "zeros"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tA7othVOnDA",
        "outputId": "e11bb6ae-f5e6-4304-a4f5-7e592539b4e8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 2., 3.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Positional Encoding"
      ],
      "metadata": {
        "id": "sfdQ3yfGFplT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Slow version"
      ],
      "metadata": {
        "id": "PSyWiT9nVXc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryPE():\n",
        "\n",
        "    def __init__(self,max_len,d_model):\n",
        "\n",
        "        self.encoding = self._precompute_binary(max_len,d_model)\n",
        "\n",
        "\n",
        "    def _get_binary(self,value):\n",
        "\n",
        "        if value == 1 or value == 0 :\n",
        "            return f\"{value}\"\n",
        "\n",
        "        binary = f\"{self._get_binary(value//2)}{value%2}\"\n",
        "\n",
        "        return binary\n",
        "\n",
        "\n",
        "    def _precompute_binary(self,max_len,d_model):\n",
        "\n",
        "        binary_table = torch.zeros((max_len,d_model))\n",
        "\n",
        "        for pos in range(max_len):\n",
        "            binary = list(map(int,self._get_binary(pos)))\n",
        "            print(binary)\n",
        "\n",
        "            binary_table[pos,d_model-len(binary):] = torch.tensor(binary)\n",
        "\n",
        "        return binary_table\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,ids):\n",
        "        return self.encoding(ids)\n"
      ],
      "metadata": {
        "id": "Es4acBwV5fZG"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "binary_pe = BinaryPE(4,6)\n",
        "binary_pe.encoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtQoQk4hQC1U",
        "outputId": "82b7f634-4faf-493d-de95-a4c61f10d619"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[1]\n",
            "[1, 0]\n",
            "[1, 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimized version"
      ],
      "metadata": {
        "id": "77fpFTYnVchx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,max_len,d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        # to move with model to device(GPU)\n",
        "        self.register_buffer('encoding', self._precompute_binary(max_len, d_model))\n",
        "\n",
        "\n",
        "    def _precompute_binary(self, max_len, d_model):\n",
        "\n",
        "        positions = torch.arange(max_len).unsqueeze(1)  # [max_len, 1]\n",
        "\n",
        "        # bit indices (msb to lsb)\n",
        "        bit_indices = torch.arange(d_model - 1, -1, -1).unsqueeze(0)  # [1, d_model]\n",
        "\n",
        "        # right shift positions by bit_indices and mask with 1\n",
        "        binary_table = (positions >> bit_indices) & 1     # braodcast tensors\n",
        "\n",
        "        return binary_table.float()\n",
        "\n",
        "\n",
        "    def forward(self,ids):\n",
        "        return self.encoding[ids]"
      ],
      "metadata": {
        "id": "l72zmL-FRwIl"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For position 5 (binary 0101):\n",
        "\n",
        "- (5 >> 3) & 1 = (0) & 1 = 0  # MSB (bit 3)\n",
        "- (5 >> 2) & 1 = (1) & 1 = 1  # bit 2\n",
        "- (5 >> 1) & 1 = (2) & 1 = 0  # bit 1\n",
        "- (5 >> 0) & 1 = (5) & 1 = 1  # LSB (bit 0)\n",
        "\n",
        "Result: [0, 1, 0, 1]"
      ],
      "metadata": {
        "id": "yxKK15feaBby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_pe = BinaryPositionalEncoding(8,12)\n",
        "binary_pe(torch.arange(0,8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRcKDqg8YZgo",
        "outputId": "633be39d-c0e4-4bf7-8ac8-7a8a5173ddab"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sinusoidal Positional Encoding"
      ],
      "metadata": {
        "id": "Hag5wJZPFtz2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NYxJhsZFwvx"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotary Positional Encoding - RoPE"
      ],
      "metadata": {
        "id": "ZDmAP4SOFxQj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NpBRmmDxF1h1"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}