{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqmg18zfqpd0hmEknHz4yO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samitha278/CoreLlama/blob/main/test_dataset_llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "nZYSGH7H8YO3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset,DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "TP8la85g9uwv"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset from Hugging Face"
      ],
      "metadata": {
        "id": "WpUZ1uljHqKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "--I_ZnnK8oAR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvjoy2uU84nd",
        "outputId": "a4d9a68d-9ecf-4087-e053-fb5621f12eaf"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 2119719\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 21990\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = dataset['train']\n",
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeWlcgO_9HBI",
        "outputId": "481f0fae-29fd-436a-c04c-ee3a491f2b80"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 2119719\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = dataset['validation']\n",
        "val_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y81bfikr9LsC",
        "outputId": "ca6171d3-a17b-408c-af5d-bf28fe421272"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 21990\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1RDUzNk9Vtc",
        "outputId": "99f77dba-609d-43b0-fab5-49d8d1ce04e8"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer -gpt2"
      ],
      "metadata": {
        "id": "3WMFuMx8HnWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "MKUATrLjECLi"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt2 = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "vugkF-ePFlYR"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_gpt2 = tokenizer_gpt2.encode(train_ds[0]['text'])\n",
        "print(tokens_gpt2)\n",
        "print(len(tokens_gpt2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjecssmRF3wR",
        "outputId": "6bac9cbb-8429-4187-fb38-101d0faad2e6"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3198, 1110, 11, 257, 1310, 2576, 3706, 20037, 1043, 257, 17598, 287, 607, 2119, 13, 1375, 2993, 340, 373, 2408, 284, 711, 351, 340, 780, 340, 373, 7786, 13, 20037, 2227, 284, 2648, 262, 17598, 351, 607, 1995, 11, 523, 673, 714, 34249, 257, 4936, 319, 607, 10147, 13, 198, 198, 43, 813, 1816, 284, 607, 1995, 290, 531, 11, 366, 29252, 11, 314, 1043, 428, 17598, 13, 1680, 345, 2648, 340, 351, 502, 290, 34249, 616, 10147, 1701, 2332, 1995, 13541, 290, 531, 11, 366, 5297, 11, 20037, 11, 356, 460, 2648, 262, 17598, 290, 4259, 534, 10147, 526, 198, 198, 41631, 11, 484, 4888, 262, 17598, 290, 384, 19103, 262, 4936, 319, 20037, 338, 10147, 13, 632, 373, 407, 2408, 329, 606, 780, 484, 547, 7373, 290, 5742, 1123, 584, 13, 2293, 484, 5201, 11, 20037, 26280, 607, 1995, 329, 7373, 262, 17598, 290, 18682, 607, 10147, 13, 1119, 1111, 2936, 3772, 780, 484, 550, 4888, 290, 3111, 1978, 13]\n",
            "162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama 2 tokenizer - community version"
      ],
      "metadata": {
        "id": "vwr3421QMMpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-hf\")"
      ],
      "metadata": {
        "id": "YErvdCZZMJz0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCOTwepQMVXd",
        "outputId": "69c5a1c5-6d24-41f4-d257-2572926d6196"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32000"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode(train_ds[0]['text'])\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMIHJXgEMZaG",
        "outputId": "289fb909-4320-4744-80bc-b91edd50ba9c"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3118, 2462, 29892, 263, 2217, 7826, 4257, 365, 2354, 1476, 263, 817, 280, 297, 902, 5716, 29889, 2296, 6363, 372, 471, 5189, 304, 1708, 411, 372, 1363, 372, 471, 15301, 29889, 365, 2354, 5131, 304, 6232, 278, 817, 280, 411, 902, 16823, 29892, 577, 1183, 1033, 409, 29893, 263, 2826, 373, 902, 528, 2728, 29889, 13, 13, 29931, 2354, 3512, 304, 902, 16823, 322, 1497, 29892, 376, 29924, 290, 29892, 306, 1476, 445, 817, 280, 29889, 1815, 366, 6232, 372, 411, 592, 322, 409, 29893, 590, 528, 2728, 3026, 2439, 16823, 25156, 322, 1497, 29892, 376, 8241, 29892, 365, 2354, 29892, 591, 508, 6232, 278, 817, 280, 322, 2329, 596, 528, 2728, 1213, 13, 13, 29911, 12966, 29892, 896, 7258, 278, 817, 280, 322, 409, 8734, 278, 2826, 373, 365, 2354, 29915, 29879, 528, 2728, 29889, 739, 471, 451, 5189, 363, 963, 1363, 896, 892, 19383, 322, 19912, 1269, 916, 29889, 2860, 896, 7743, 29892, 365, 2354, 6452, 287, 902, 16823, 363, 19383, 278, 817, 280, 322, 27826, 902, 528, 2728, 29889, 2688, 1716, 7091, 9796, 1363, 896, 750, 7258, 322, 3796, 4208, 29889]\n",
            "185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Special Tokens:\n",
        "\n",
        "<s> (BOS - Beginning of Sequence) - Marks the start of text\n",
        "</s> (EOS - End of Sequence) - Marks the end of text\n",
        "<unk> (Unknown) - Represents unknown/out-of-vocabulary words\n",
        "<pad> (Padding) - Used to pad sequences to the same length\n",
        "\n",
        "\n",
        "When tokenize text with add_special_tokens=True :\n",
        "\n",
        "text = \"Hello world\"\n",
        "tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "# Result: [1, 15043, 3186, 2]\n",
        "# Where: 1 = <s>, 15043 = \"Hello\", 3186 = \"world\", 2 = </s>\n",
        "\n",
        "\n",
        "With add_special_tokens=False:\n",
        "\n",
        "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "# Result: [15043, 3186]\n",
        "# Just the actual words, no BOS/EOS tokens\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "njm7TUzBQ17L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset wrapper"
      ],
      "metadata": {
        "id": "Y_fLgMPxHwfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self,dataset,tokenizer,max_len):\n",
        "\n",
        "        self.dataset = dataset\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        text = self.dataset[idx]['text']\n",
        "\n",
        "        # tokenize\n",
        "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "        l = len(tokens)\n",
        "\n",
        "        # truncate if tokens long\n",
        "        if self.max_len < l:\n",
        "            tokens = tokens[:self.max_len]\n",
        "        else: # pad (if max_len > l)\n",
        "            tokens = tokens + [self.tokenizer.pad_token_id] * (self.max_len-l)\n",
        "\n",
        "        # convert to tensor\n",
        "        tokens = torch.tensor(tokens)\n",
        "        x = tokens[:-1]\n",
        "        y = tokens[1:]\n",
        "\n",
        "        return x,y"
      ],
      "metadata": {
        "id": "Bqj1JbLIGwOZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "X9X0h5XbS0_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 512"
      ],
      "metadata": {
        "id": "FJTOs-U-YIXo"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging face dataset\n",
        "train_ds= dataset['train']\n",
        "val_ds = dataset['validation']"
      ],
      "metadata": {
        "id": "QfXHhMbISZbY"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset wrapper\n",
        "\n",
        "train_ds = TextDataset(train_ds,tokenizer,max_len)\n",
        "val_ds = TextDataset(val_ds,tokenizer,max_len)"
      ],
      "metadata": {
        "id": "n9ZAhCnHX2dU"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_ds,batch_size=4,shuffle = True)   # by calling train_ds[idx] get 4 (batch size) text rows and concat and prepare batch\n",
        "val_loader = DataLoader(val_ds,batch_size=4,shuffle = True)"
      ],
      "metadata": {
        "id": "_InTsayGXQXI"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwuwJ-_fXoLK",
        "outputId": "9aa7a439-fb66-44b8-c45f-fe43f9ce98d9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[    1,   612, 18358,  ...,     0,     0,     0],\n",
              "         [    1,  9038,  2501,  ...,     0,     0,     0],\n",
              "         [    1,  9038,  2501,  ...,     0,     0,     0],\n",
              "         [    1,  9038,   727,  ...,     0,     0,     0]]),\n",
              " tensor([[  612, 18358, 29892,  ...,     0,     0,     0],\n",
              "         [ 9038,  2501,   263,  ...,     0,     0,     0],\n",
              "         [ 9038,  2501,   263,  ...,     0,     0,     0],\n",
              "         [ 9038,   727,   471,  ...,     0,     0,     0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UsMpZuQ4bE3q"
      },
      "execution_count": 84,
      "outputs": []
    }
  ]
}